{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ARMA and GARCH processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "# Chapter 1: Time Series Fundamentals\n",
    "***\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working with ARIMA models, let us introduce/refresh some terminology first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocovariance and Autocorrelation\n",
    "\n",
    "<br>\n",
    "\n",
    "Let $(X_t)_{t \\in \\mathbb{Z}}$ be a (discrete) stochastic process. In order to measure the dependency structure\n",
    "to itself but lagged manner, we define the __autocovariance__ function $\\gamma(t,s)$ as\n",
    "\n",
    "<br>\n",
    "$$ \\gamma(s,t) = \\mathbb{E}[(X_t - \\mu_t)(X_s - \\mu_s)] $$\n",
    "<br>\n",
    "\n",
    "and the __autocorrelation__ $\\rho(s,t)$ function as\n",
    "\n",
    "<br>\n",
    "$$ \\rho(s,t) = \\frac{\\gamma(s,t)}{\\sqrt{\\gamma(s,s)}\\sqrt{\\gamma(t,t)}}$$\n",
    "<br>\n",
    "\n",
    "both functions are a way to measure the linear(!) dependence of the variable given different time periods.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "\n",
    "<br>\n",
    "\n",
    "As we have seen in the previous lectures, prices behave randomly, it is not clear how they evolve over time.\n",
    "In particular, it is impossible to infer any asymptotic behaviour. In order to to predictions we need some\n",
    "sort of 'stability' criterion. For meaningful predictions using ARMA processes, this property is called stationarity.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let $(X_t)_{t \\in \\mathbb{Z}}$ be a (discrete) stochastic process. The process is called\n",
    "\n",
    "- __mean stationary__ $: \\Leftrightarrow$ for all $t \\in T:$ $\\mathbb{E}[X_t] = \\mu$\n",
    "- __variance stationary__ $: \\Leftrightarrow$ for all $t \\in T:$ $\\mathbb{V}[X_t] = \\sigma^2$\n",
    "- __covariance stationary__: $: \\Leftrightarrow$ for all $s, t, h \\in T:$ $\\gamma(s,t) = \\gamma(s+h,t+h)$\n",
    "\n",
    "A process which is mean and covariance stationary is also called __weakly stationary__.\n",
    "\n",
    "<br>\n",
    "\n",
    "For a stationary time series, the autocovariance is denoted by\n",
    "\n",
    "<br>\n",
    "$$\\gamma(h) = \\mathbb{E}[(X_t - \\mu)(X_s - \\mu)] \\qquad \\text{and}$$\n",
    "<br>\n",
    "\n",
    "and the autocorrelation can be simplified to\n",
    "\n",
    "<br>\n",
    "$$\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)}$$\n",
    "<br>\n",
    "\n",
    "Stationarity can be statistically tested using so called unit-root tests, e.g. Augmented-Dickey-Fuller test. We will\n",
    "review some statistical testing in the later stages of this lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Exercise: Gaussian White Noise\n",
    "\n",
    "Consider a Gaussian White Noise process $(\\varepsilon_t)_{t \\in \\mathbb{Z}}$. Recall, the variates of\n",
    "a gaussian white noise process are independent and identically distributed, formally\n",
    "\n",
    "<br>\n",
    "$$ \\varepsilon_t \\stackrel{IID}{\\sim} N(0, \\sigma_\\varepsilon^2) \\quad \\forall t \\in \\mathbb{Z}$$\n",
    "<br>\n",
    "\n",
    "In the following, simulate a Gaussian White Noise process and review the autocorrelation function.\n",
    "Explore the numpy random module, i.e. np.random..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gaussian_white_noise = np.random.normal(loc=0, scale=1, size=500)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "plt.plot(gaussian_white_noise)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "ACF_GWN = plot_acf(gaussian_white_noise)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since Gaussian White Noise is iid by definition, it is not surprising to see that the lags have little to no\n",
    "impact on the future values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Random Walk (Exercise)\n",
    "\n",
    "Let us consider the opposite in which the time series is by definition dependent from the past.\n",
    "\n",
    "Let $(\\varepsilon_t)_{t \\in \\mathbb{Z}}$ be a gaussian white noise process. The simple symmetric random\n",
    "walk process is defined as\n",
    "\n",
    "<br>\n",
    "$$ y_t = y_{t-1} + \\varepsilon_t$$\n",
    "<br>\n",
    "\n",
    "Note: For simplicity, the constant is kept at zero. The constant is also called a trend.\n",
    "\n",
    "Simulate a random walk and evaluate the autocorrelation function.\n",
    "\n",
    "Hint: Use the `np.cumsum` function in order to generate the random walk process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "gaussian_innovation = np.random.normal(loc=0, scale=1, size=500)\n",
    "gaussian_innovation[0]=0\n",
    "\n",
    "RW = np.cumsum(gaussian_innovation)\n",
    "\n",
    "plt.plot(RW)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "ACF_RW = plot_acf(RW)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "# Chapter 2: ARMA Processes\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "Autoregressive Moving Average models belong to the parametric family of stationary time series.\n",
    "ARMA models are in particular important since it is possible to approximate a large class of\n",
    "series in terms of autocovariance functions. [3, p.73]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fundamentals\n",
    "\n",
    "<br>\n",
    "\n",
    "Let $(\\varepsilon_t)_{t \\in \\mathbb{Z}}$ be a general white noise process.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "An __Autoregressive Model of order p__ or __AR(p)__ model is of the form\n",
    "\n",
    "<br>\n",
    "$$X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + ... + \\phi_p X_{t-p} + \\varepsilon_t  $$\n",
    "<br>\n",
    "\n",
    "Hence, an autoregressive model is a weighted sum of the past values.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "A __Moving Average Model of order q__ or __MA(q)__ is defined by\n",
    "\n",
    "<br>\n",
    "$$X_t = \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + ... + \\theta_q \\varepsilon_{t-q} + \\varepsilon_t  $$\n",
    "<br>\n",
    "\n",
    "We can see that in this case, the model is determined by the weighted sum of the past innovations and not their\n",
    "past values themselves.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "An __Autoregressive Moving Average Process of order p and q__ or __ARMA(p,q)__ is the summary of both previous\n",
    "models, i.e.\n",
    "\n",
    "<br>\n",
    "$$X_t - \\phi_tX_{t-1} - ... - \\phi_pX_{t-p} = \\varepsilon_t + \\theta_t\\varepsilon_{t-1} + ... \\theta_q\\varepsilon_{t-q} $$\n",
    "<br>\n",
    "\n",
    "This can model can be written in a more convenient way by using a backshift operator B.\n",
    "\n",
    "<br>\n",
    "$$B(x_t) = Bx_{t-1} = x_{t-1} $$\n",
    "<br>\n",
    "\n",
    "the ARMA model can be rewritten in the form of\n",
    "\n",
    "<br>\n",
    "$$ \\phi(B)X_t = \\theta(B)\\varepsilon_t$$\n",
    "<br>\n",
    "\n",
    "where $\\phi(\\cdot)$ and $\\theta(\\cdot)$ are the p-th and q-th-degree polynomials, that is\n",
    "\n",
    "<br>\n",
    "$$ \\phi(x) = 1-\\phi_1 x - ... -\\phi_p x^p $$\n",
    "<br>\n",
    "\n",
    "and\n",
    "\n",
    "<br>\n",
    "$$ \\theta(x) = 1 + \\theta_1 x + ... + \\theta_q x^q $$\n",
    "<br>\n",
    "\n",
    "simply insert the backshift operator in the polynomial and multiply it with $X_t$.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Example: ARMA(1,1) process\n",
    "\n",
    "The coefficients follow the notation used in the corresponding polynomials which includes\n",
    "1 as the zero-lag, i.e. $X_t$, component. AR parameters also have opposite signs!\n",
    "\n",
    "<br>\n",
    "$$X_t - \\phi_1 X_{t-1} = \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}$$\n",
    "<br>\n",
    "\n",
    "which is the same as\n",
    "\n",
    "<br>\n",
    "$$X_t = \\phi_1 X_{t-1} + \\theta_1 \\varepsilon_{t-1} + \\varepsilon_t$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: ARMA process\n",
    "\n",
    "Simulate an ARMA(1,1) process. Explore the functionality of the `arima_process` module. You\n",
    "will need the `.arma_generate_sample` method. After generating the sample, generate\n",
    "a datetime range of your choice and match both time series into a pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa import arima_process\n",
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phi_1 = -0.9\n",
    "theta_1 = 0.1\n",
    "\n",
    "ar_coefs = [1, phi_1]\n",
    "ma_coefs = [1, theta_1]\n",
    "\n",
    "sample = arima_process.arma_generate_sample(ar_coefs, ma_coefs, nsample=365)\n",
    "dti = pd.date_range(\"2020-01-01\", periods=len(sample), freq=\"d\")\n",
    "arma_sample = pd.Series(sample, index=dti)\n",
    "\n",
    "plt.plot(arma_sample)\n",
    "plt.show()\n",
    "\n",
    "# Exercise: Check the documentation. Which white noise process is used and what is it's standard deviation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "There are three common ways to estimate the parameters of an ARMA(p,q) process.\n",
    "\n",
    "- Yule-Walker Estimation (Method of Moments)\n",
    "- Least-Square Estimation\n",
    "- Maximum-Likelihood Estimation\n",
    "\n",
    "We will focus on the Maximum-Likelihood estimator in order to derive the parameters since this is the most common method. However, it requires a specific distribution assumption for the error term $\\varepsilon_t$.\n",
    "\n",
    "A quick reminder, given an ARMA(p,q) model $X_t$, we need to derive the likelihood function which is of the form\n",
    "\n",
    "<br>\n",
    "$$ L(\\beta, \\sigma_w^2)= \\prod\\limits_{i=1}^{n}f_{X}(x_t|x_{t-1},...,x_1)$$\n",
    "<br>\n",
    "\n",
    "where $\\beta=(\\mu, \\phi_1,...\\phi_p, \\theta_1,...,\\theta_q)'$ denotes the vector of parameters. Naturally, the densities depends on the distribution we assume for the white-noise variates. For an explicite derivation of the quantities, see [1, p.116].\n",
    "\n",
    "Two common numerical optimization routines for computing the maximum likelihood estimator are the Newton-Raphson and Scoring algorithm. For more details, see [1, p.120]\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Fit an ARIMA(1,0,1) = ARMA(1,1) model\n",
    "model = ARIMA(arma_sample, order=(1,0,1))\n",
    "\n",
    "model_fit = model.fit()\n",
    "\n",
    "print(model_fit.summary())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_diagonistics = model_fit.plot_diagnostics(figsize=(15, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Forecasting\n",
    "\n",
    "Based on a given set of data, we want to predict a future value. Formally,\n",
    "\n",
    "<br>\n",
    "$$ x_{n+m}^{(n)} = g(x_1,...,x_n)$$\n",
    "<br>\n",
    "\n",
    "where $g(\\cdot)$ is some function used for prediction and $x_{n+m}^{(n)}$ denotes the m-step forecast based on n-past values.\n",
    "\n",
    "Once $g(\\cdot)$ is determined and the forecast made. The predicted value needs to be evaluated and compared to the true/realized value. Keep in mind, that the accuracy of a forecast is always tied to a loss function $l(\\cdot)$, i.e.\n",
    "\n",
    "<br>\n",
    "$$ l(x_{(n+m)}, x_{n+m}^{(n)})$$\n",
    "<br>\n",
    "\n",
    "In this lecture, we assume that our models are stationary. We will focus on the class of linear predictors which are of the form\n",
    "\n",
    "<br>\n",
    "$$ x_{n+m}^{(n)} = \\alpha_0 + \\sum\\limits_{k=1}^{n}\\alpha_kx_k$$\n",
    "<br>\n",
    "\n",
    "and set the loss function to be the mean square error\n",
    "\n",
    "<br>\n",
    "$$ S(\\alpha_0,...,\\alpha_n) = l(x_{(n+m)}, x_{n+m}^{(n)}) = \\mathbb{E}[(x_{n+m} - x_{n+m}^{(n)})^2] $$\n",
    "<br>\n",
    "\n",
    "Given the data, the best linear predictor $x_{n+m}^{(n)}$ for $m\\geq1$ is found by solving\n",
    "\n",
    "<br>\n",
    "$$\\mathbb{E}[(x_{n+m}-x^{(n)}_{n+m})x_k]=0 \\qquad k=0,...,n$$\n",
    "<br>\n",
    "\n",
    "this set of equations are also called prediction equations and originate from the partial derivatives of the loss function with respect to the parameters $\\alpha_0,...,\\alpha_n$. The predictor is also called Best-Linear-Predictor (BLP).\n",
    "\n",
    "The Durbin-Levinson Algorithm is commonly applied to solve the prediction equations.[3, p.60]\n",
    "\n",
    "Note that forecasting in terms of coding is referred to the predicted mean.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Suppose we want to forecast some values using the ARMA model built from the previous exercises. First, check\n",
    "the in-sample prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred_in = model_fit.get_prediction(start=arma_sample.index[0], end=arma_sample.index[len(arma_sample)-1])\n",
    "\n",
    "pred_in_conf = pred_in.conf_int(alpha=0.05)\n",
    "pred_in_mean = pred_in.predicted_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(21,9)\n",
    "\n",
    "ax.set_title('In-sample Prediction', fontsize = 12)\n",
    "ax.set_xlabel('Days')\n",
    "ax.set_ylabel('Values')\n",
    "ax.grid(color='grey', linestyle='--', linewidth=0.5)\n",
    "ax.plot(pred_in_mean)\n",
    "ax.plot(pred_in_conf)\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Thus, the prediction in sample seem to be ok. The one-step ahead forecast is then given by"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Define prediction horizon\n",
    "begin = arma_sample.index[len(arma_sample)-1]\n",
    "steps_ahead = datetime.timedelta(days=7)\n",
    "\n",
    "pred_out = model_fit.get_prediction(start=begin, end= begin+steps_ahead)\n",
    "\n",
    "pred_out_conf = pred_out.conf_int(alpha=0.05)\n",
    "pred_out_mean = pred_out.predicted_mean"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore the mechanics in a more detailed way in the case study following this lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Chapter 3 : ARCH and GARCH processes\n",
    "***\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fundamentals\n",
    "\n",
    "<br>\n",
    "\n",
    "In contrast to ARMA models which assume the variance to be constant, ARCH and GARCH models\n",
    "allow for time dependent variance. There are also called heteroscedastic models. We will see,\n",
    "that this process is much better in capturing volatility clusters.\n",
    "\n",
    "<br>\n",
    "\n",
    "Recall:\n",
    "\n",
    "* Volatility is not directly observable\n",
    "* return series are not serially correlated, however, squared and absolute returns show profound serial\n",
    "  correlation\n",
    "* volatility clustering\n",
    "\n",
    "<br>\n",
    "\n",
    "### Definition (ARCH process)\n",
    "Let $(\\varepsilon_t)_{t \\in \\mathbb{Z}}$ be a standard white noise process with $\\varepsilon_t \\stackrel{IID}{\\sim}\n",
    "  \\text{WN}(0,1)$. The process $(x_t)_{t \\in \\mathbb{Z}}$ is an ARCH(p) process iff\n",
    "\n",
    "\\begin{align}\n",
    "    x_t &= \\sigma_t\\varepsilon_t \\\\\n",
    "    \\sigma_t^2 &= \\alpha_0 + \\sum_{i=1}^{p}\\alpha_i x^2_{t-i}\n",
    "\\end{align}\n",
    "\n",
    "with $\\alpha_0 > 0$ and $\\alpha_i \\geq 0$ for $i=1,...,p$. The latter restrictions are required because the variance needs to be positive.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Some properties of ARCH processes\n",
    "\n",
    "* An ARCH(p) process is covariance/weakly stationary if: $\\quad \\sum\\limits_{i=1}^p \\alpha_i < 1$ <br/>\n",
    "  If the process is covariance stationary, i.e. stable, the **unconditional** variance is\n",
    "  well-defined and given by\n",
    "\n",
    "  \\begin{equation} \\mathbb{V}[x_t] = \\sigma^2 = \\dfrac{\\alpha_0}{1 - \\sum\\limits_{i=1}^p \\alpha_i} \\end{equation}\n",
    "\n",
    "  The method for computing the unconditional variance is to reformulate the model into a stochastic recurrence equation. Basically, the ARCH process is represented as an ARMA process with $x_{t}^2$ (see [1], p.141). <br/>\n",
    "\n",
    "* If $x_t$ has finite first moments, the process has the martingale difference property, $\\mathbb{E}[x_t|\\mathcal{F}_{t-1}]=0$, w.r.t the $\\sigma$-algebra generated by $x_t$. In particular, the unconditional mean is zero.\n",
    "\n",
    "* One can show that the kurtosis of an ARCH process is larger than 3 assuming the fourth\n",
    "moment is well-defined. This is desirable since a kurtosis $>3$ implies a leptocurtic distribution\n",
    "\n",
    "* A potential drawback is the requirement of many parameters, i.e. many lags, to model the volatility properly (see ACF and PACF). This problem can be circumvented by including past variance variates instead of past returns (see GARCH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Example: ARCH(1)\n",
    "\n",
    "With $(\\varepsilon_t)_{t \\in \\mathbb{Z}}$ as standard gaussian white noise process with\n",
    "$\\varepsilon_t \\stackrel{IID}{\\sim}  \\text{N}(0,1)$. An ARCH(1) $(x_t)_{t \\in \\mathbb{Z}}$ process is of the form\n",
    "\n",
    "\\begin{align}\n",
    "    x_t &= \\sigma_t\\varepsilon_t \\\\\n",
    "    \\sigma_t^2 &= \\alpha_0 + \\alpha_{1} x^2_{t-1}\n",
    "\\end{align}\n",
    "\n",
    "with $\\alpha_0, \\alpha_1 > 0$. The conditional density $x_t|x_{t-1}$ given normal distribution is given by\n",
    "\n",
    "$$ x_t|x_{t-1} \\sim N(0, \\alpha_0 + \\alpha_1 x^2_{t-1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Exercise: Simulate ARCH(1)\n",
    "\n",
    "Simulate an ARCH(1) process. Define the parameters first. Optionally, preallocate some space\n",
    "by initializing two vectors of zeros/ones. Generate the time series with a loop. Plot\n",
    "the time series."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Define some parameters\n",
    "a_0 = 0.3\n",
    "a_1 = 0.3\n",
    "\n",
    "# Tryout unstable version\n",
    "\n",
    "n = 500\n",
    "\n",
    "# Preallocate arrays\n",
    "wn = np.random.normal(size=n)           # white noise\n",
    "x = np.zeros(n)                         # final time series\n",
    "sigma_square = np.zeros(n)              # sigma^2\n",
    "\n",
    "\n",
    "for i in range(1, n):\n",
    "    sigma_square[i] = a_0 + a_1*(x[i-1]**2)\n",
    "    x[i] = wn[i]*np.sqrt(sigma_square[i])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "fig.set_size_inches(10, 6)\n",
    "ax.set_title('ARCH(1)', fontsize = 20)\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_ylabel('Values')\n",
    "ax.grid(color='grey', linestyle='--', linewidth=0.5)\n",
    "plt.plot(x)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One natural extension is to include past observation of the variance which leads to the following definition.\n",
    "\n",
    "### Definition (GARCH process)\n",
    "Let $(\\varepsilon_t)_{t \\in \\mathbb{Z}}$ be standard white noise, i.e. $\\varepsilon_t \\stackrel{IID}{\\sim}  WN(0,1)$.\n",
    "The process $(x_t)_{t \\in \\mathbb{Z}}$ is an GARCH(p,q) process if\n",
    "\n",
    "\\begin{align}\n",
    "    x_t &= \\sigma_t\\varepsilon_t \\\\\n",
    "    \\sigma_t^2 &= \\alpha_0 + \\sum_{i=1}^{p}\\alpha_i x^2_{t-i} + \\sum_{j=1}^{p}\\beta_i \\sigma^2_{t-j}\n",
    "\\end{align}\n",
    "\n",
    "with $\\alpha_0 > 0$ and $\\alpha_i \\geq 0$ for $i=1,...,p$ and $\\beta_i \\geq 0$ for $j=1,...,q$.\n",
    "\n",
    "The history of variance is incorporated to this model.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Some properties of GARCH processes\n",
    "\n",
    "* High phases of volatility tend to be more **persistent** since $x_t$ is likely to be larger if either $|x_{t-1}|$ or $\\sigma_{t-1}$ are large\n",
    "\n",
    "* A GARCH(p,q) process is covariance stationary if: $\\quad \\sum\\limits_{i=1}^p \\alpha_i + \\sum\\limits_{j=1}^q \\beta_j < 1$ <br/>\n",
    "  This quantity (the sum above) can be seen as a **persistence measure** of shocks to the volatility.\n",
    "  Similarly, the rate of how fast the past effects and shocks decay. The closer to one, the longer it persists.\n",
    "  If this is case, similar to above, the unconditional variance is well-defined and is given by\n",
    "\n",
    "  $$\\quad \\mathbb{V}[x_t] = \\sigma^2 = \\dfrac{\\alpha_0}{1 - \\sum\\limits_{i=1}^p \\alpha_i - \\sum\\limits_{j=1}^q \\beta_j }$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Example: GARCH(1,1)\n",
    "\n",
    "With $(\\varepsilon_t)_{t \\in \\mathbb{Z}}$ as standard gaussian white noise process with $\\varepsilon_t \\stackrel{IID}{\\sim}  \\text{N}(0,1)$. An ARCH(1) $(x_t)_{t \\in \\mathbb{Z}}$ process is of the form\n",
    "\n",
    "\\begin{align}\n",
    "    x_t &= \\sigma_t\\varepsilon_t \\\\\n",
    "    \\sigma_t^2 &= \\alpha_0 + \\alpha_{1} x^2_{t-1} + \\beta_1\\sigma^2_{t-1}\n",
    "\\end{align}\n",
    "\n",
    "with $\\alpha_0, \\alpha_1 > 0$. The conditional density $x_t|x_{t-1}$ given normal distribution is given by\n",
    "\n",
    "$$ x_t|x_{t-1} \\sim N(0, \\alpha_0 + \\alpha_1 x^2_{t-1}+ \\beta_1\\sigma^2_{t-1})$$\n",
    "\n",
    "Let's simulate the process:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Define some parameters\n",
    "a_0 = 0.2\n",
    "a_1 = 0.5\n",
    "b_1 = 0.5\n",
    "\n",
    "# Tryout persistent and unstable combinations\n",
    "\n",
    "# Sample size\n",
    "n = 365\n",
    "\n",
    "# Preallocate arrays\n",
    "wn = np.random.normal(size=n)           # white noise\n",
    "x = np.zeros(n)                         # final time series\n",
    "sigma_square = np.zeros(n)              # sigma^2\n",
    "\n",
    "\n",
    "for i in range(1, n):\n",
    "    sigma_square[i] = a_0 + a_1*(x[i-1]**2) + b_1*sigma_square[i-1]\n",
    "    x[i] = wn[i]*np.sqrt(sigma_square[i])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "fig.set_size_inches(10, 6)\n",
    "ax.set_title('GARCH(1,1)', fontsize = 20)\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_ylabel('Values')\n",
    "ax.grid(color='grey', linestyle='--', linewidth=0.5)\n",
    "plt.plot(x)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Estimation\n",
    "\n",
    "### Conditional Densities\n",
    "\n",
    "Estimation is usually done with the Maximum-Likelihood method. For that, we have to study the conditional distribution of $x_t$ first.\n",
    "\n",
    "Given two random variables X,Y, the joint densitiy can be expressed in terms of conditional densities\n",
    "\n",
    "$$ f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_X(x)} \\quad \\Leftrightarrow \\quad f_{X,Y}(x,y) = f_{X|Y}(x|y) * f_X(x)$$\n",
    "\n",
    "Based on this principle the joint distribution of a collection of random variables $X_0,...,X_T$ can be written as\n",
    "\n",
    "$$ f_{X_0,...,X_T}(x_0,...,x_T) = f_{X_0}(x_0) \\prod_{i=1}^{T} f_{X_t|X_{t-1},...,X_0} (x_t | x_{t-1},...,x_0)$$\n",
    "\n",
    "The analytical expression of the conditional density $f_{X_t|X_{t-1},...,X_0} (x_t | x_{t-1},...,x_0)$ depends on the model, that is the lag order and distribution assumption.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Constructing a likelihood function\n",
    "\n",
    "Again, take a GARCH(1,1) process with standard gaussian white noise. Let $ X= (X_1, ...,X_N)$ denote a collection of random variables. The likelihood function is then of the form\n",
    "\n",
    "$$ \\mathcal{L}(\\alpha_0,\\alpha_1,\\beta_1; X) = f_{X_0}(x_0) * \\prod_{t=1}^{N}f_{X_t|X_{t-1}}(x_t|x_{t-1})$$\n",
    "\n",
    "Usually $f_{X_0}(x_0)$ is not known. However, the effect of this term is negliable if the sample size is large therefore it is moslty droped. The resulting function is also called the conditional likelihood function. With Gaussian innovations, the explicite form of the conditional likelihood function is given by\n",
    "\n",
    "$$ L(\\alpha_0,\\alpha_1,\\beta_1; X) = \\prod_{t=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma_t^{2}}}\\text{exp}(-\\frac{x_t^2}{2\\sigma_t^2}) \\qquad \\text{with} \\quad \\sigma_t^2=\\alpha_0+\\alpha_1 x_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2$$\n",
    "\n",
    "The log-likelihood function is given by\n",
    "\n",
    "$$ ln(L(\\alpha_0,\\alpha_1,\\beta_1; X)) = \\frac{N}{2}ln(2\\pi)-\\frac{1}{2}\\sum_{t=1}^{N}ln(\\sigma_t^2)-\\frac{1}{2}\\sum_{t=1}^{N}\\frac{x_t^2}{\\sigma_t^2}$$\n",
    "\n",
    "This function can now be optimized in order to derive the parameters.\n",
    "\n",
    "Note, that $\\sigma_t^2$ and $x_t$ are recursively defined, thus $\\sigma_0^2$ and $x_0$ needs to be pre-determined. For $\\sigma_0$  the sample variance is mostly usedand $x_0$ is the corresponding value at that given time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's do some estimation on the data given from the previous task."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from arch import arch_model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "dti = pd.date_range(\"2020-01-01\", periods=len(x), freq=\"d\")\n",
    "garch_sample = pd.Series(x, index=dti)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_lag = 1\n",
    "q_lag = 1\n",
    "\n",
    "garch_model = arch_model(garch_sample, p=p_lag, q=q_lag)\n",
    "garch_model_fit = garch_model.fit(update_freq=0, disp='off')\n",
    "\n",
    "print(garch_model_fit.summary())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### References\n",
    "\n",
    "#### Chapter 1 - Chapter 2\n",
    "\n",
    "\n",
    "[1] Shumway, Stoffer (2017) - Time Series Analysis and Its Applications - With R Examples\n",
    "\n",
    "[2] Brockwell, Davis (2006) - Time Series - Theory and Methods\n",
    "\n",
    "[3] Brockwell, Davis (2016) - Introduction to Time Series and Forecasting\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Chapter 3\n",
    "\n",
    "[1] McNeil, Frey, Embrechts,(2005), p.139-145\n",
    "\n",
    "[2] LÃ¼tkepohl (2007), p.559-560\n",
    "\n",
    "[3] Tsay (2010), p.109-171\n",
    "\n",
    "\n",
    "[A] A nice example with many features on forecasting\n",
    "https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}